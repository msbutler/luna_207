
@article{luna,
	title = {Learned {Uncertainty}-{Aware} ({LUNA}) {Bases} for {Bayesian} {Regression} using {Multi}-{Headed} {Auxiliary} {Networks}},
	url = {http://arxiv.org/abs/2006.11695},
	abstract = {Neural Linear Models (NLM) are deep models that produce predictive uncertainty by learning features from the data and then performing Bayesian linear regression over these features. Despite their popularity, few works have focused on formally evaluating the predictive uncertainties of these models. In this work, we show that traditional training procedures for NLMs can drastically underestimate uncertainty in data-scarce regions. We identify the underlying reasons for this behavior and propose a novel training procedure for capturing useful predictive uncertainties.},
	urldate = {2020-11-01},
	journal = {arXiv:2006.11695 [cs, stat]},
	author = {Thakur, Sujay and Lorsung, Cooper and Yacoby, Yaniv and Doshi-Velez, Finale and Pan, Weiwei},
	month = jul,
	year = {2020},
	note = {arXiv: 2006.11695},
	annote = {Comment: ICML 2020 Workshop on Uncertainty and Robustness in Deep Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\jscud\\Zotero\\storage\\SKIZJPML\\Thakur et al. - 2020 - Learned Uncertainty-Aware (LUNA) Bases for Bayesia.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jscud\\Zotero\\storage\\3U9CQR9W\\2006.html:text/html},
}

@article{adam,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2020-11-05},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv Fulltext PDF:C\:\\Users\\jscud\\Zotero\\storage\\BC7DTMPV\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jscud\\Zotero\\storage\\9CIQ39VU\\1412.html:text/html},
}

@book{neal,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Bayesian {Learning} for {Neural} {Networks}},
	volume = {118},
	isbn = {978-0-387-94724-2 978-1-4612-0745-0},
	url = {http://link.springer.com/10.1007/978-1-4612-0745-0},
	abstract = {Two features distinguish the Bayesian approach to learning models from data. First, beliefs derived from background knowledge are used to select a prior probability distribution for the model parameters. Second, predictions of future observations are made by integrating the model's predictions with respect to the posterior parameter distribution obtained by updating this prior to take account of the data. For neural network models, both these aspects present di culties {\textbar} the prior over network parameters has no obvious relation to our prior knowledge, and integration over the posterior is computationally very demanding.},
	language = {en},
	urldate = {2020-12-13},
	publisher = {Springer New York},
	author = {Neal, Radford M.},
	editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
	year = {1996},
	doi = {10.1007/978-1-4612-0745-0},
}

@article{snoek,
	title = {Scalable {Bayesian} {Optimization} {Using} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1502.05700},
	abstract = {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.},
	urldate = {2020-12-13},
	journal = {arXiv:1502.05700 [stat]},
	author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Md Mostofa Ali and Prabhat and Adams, Ryan P.},
	month = jul,
	year = {2015},
	note = {arXiv: 1502.05700},
	file = {arXiv Fulltext PDF:C\:\\Users\\jscud\\Zotero\\storage\\AAYAFX5Z\\Snoek et al. - 2015 - Scalable Bayesian Optimization Using Deep Neural N.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jscud\\Zotero\\storage\\AHIC8YDQ\\1502.html:text/html},
}

@article{zhou,
	title = {Adaptive {Bayesian} {Linear} {Regression} for {Automated} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1904.00577},
	abstract = {To solve a machine learning problem, one typically needs to perform data preprocessing, modeling, and hyperparameter tuning, which is known as model selection and hyperparameter optimization.The goal of automated machine learning (AutoML) is to design methods that can automatically perform model selection and hyperparameter optimization without human interventions for a given dataset. In this paper, we propose a meta-learning method that can search for a high-performance machine learning pipeline from the predefined set of candidate pipelines for supervised classification datasets in an efficient way by leveraging meta-data collected from previous experiments. More specifically, our method combines an adaptive Bayesian regression model with a neural network basis function and the acquisition function from Bayesian optimization. The adaptive Bayesian regression model is able to capture knowledge from previous meta-data and thus make predictions of the performances of machine learning pipelines on a new dataset. The acquisition function is then used to guide the search of possible pipelines based on the predictions.The experiments demonstrate that our approach can quickly identify high-performance pipelines for a range of test datasets and outperforms the baseline methods.},
	urldate = {2020-12-13},
	journal = {arXiv:1904.00577 [cs, stat]},
	author = {Zhou, Weilin and Precioso, Frederic},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.00577},
	annote = {Comment: Added references;Corrected typos.Revised argument,results unchanged},
	file = {arXiv Fulltext PDF:C\:\\Users\\jscud\\Zotero\\storage\\5HR4IFI3\\Zhou and Precioso - 2019 - Adaptive Bayesian Linear Regression for Automated .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jscud\\Zotero\\storage\\XYXDUVU8\\1904.html:text/html},
}

@book{tibshirani,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {An {Introduction} to {Statistical} {Learning}},
	volume = {1},
	isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
	url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
	language = {en},
	urldate = {2020-12-13},
	publisher = {Springer New York},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	doi = {10.1007/978-1-4614-7138-7},
	file = {James et al. - 2013 - An Introduction to Statistical Learning.pdf:C\:\\Users\\jscud\\Zotero\\storage\\5GHVTXPE\\James et al. - 2013 - An Introduction to Statistical Learning.pdf:application/pdf},
}

@article{hill,
	chapter = {Technology},
	title = {Wrongfully {Accused} by an {Algorithm}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html},
	abstract = {In what may be the first known case of its kind, a faulty facial recognition match led to a Michigan manâ€™s arrest for a crime he did not commit.},
	language = {en-US},
	urldate = {2020-12-14},
	journal = {The New York Times},
	author = {Hill, Kashmir},
	month = jun,
	year = {2020},
	file = {Snapshot:C\:\\Users\\jscud\\Zotero\\storage\\V6BM7Y99\\facial-recognition-arrest.html:text/html},
}

@misc{kendall,
	title = {Deep {Learning} {Is} {Not} {Good} {Enough}, {We} {Need} {Bayesian} {Deep} {Learning} for {Safe} {AI}},
	url = {//alexgkendall.com/computer_vision/bayesian_deep_learning_for_safe_ai/},
	abstract = {Bayesian Deep Learning, Computer Vision, Uncertainty},
	language = {en},
	urldate = {2020-12-14},
	journal = {Home},
	author = {Kendall, Alex},
	month = may,
	year = {2017},
	file = {Snapshot:C\:\\Users\\jscud\\Zotero\\storage\\79JZWU5C\\bayesian_deep_learning_for_safe_ai.html:text/html},
}