{"cells":[{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00000-4de55159-dd7e-48a9-baf0-87d8dd2876c1","output_cleared":false,"source_hash":"de8f455b","execution_millis":1299,"execution_start":1606149332881},"source":"!pip install autograd\nfrom autograd import numpy as np\nfrom autograd import grad\nfrom autograd.misc.optimizers import adam, sgd\nfrom autograd import scipy as sp\nimport autograd.numpy.random as npr\nimport pandas as pd\nimport numpy\nimport matplotlib.pyplot as plt\nimport sys\n\nimport bayes_helpers as bh","execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: autograd in /opt/venv/lib/python3.7/site-packages (1.3)\r\nRequirement already satisfied: numpy>=1.12 in /opt/venv/lib/python3.7/site-packages (from autograd) (1.19.4)\r\nRequirement already satisfied: future>=0.15.2 in /opt/venv/lib/python3.7/site-packages (from autograd) (0.18.2)\r\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00000-27450806-b6c6-4fb7-ab3a-0921fd5243c9","output_cleared":false,"source_hash":"83498f2b","execution_millis":42,"execution_start":1606150909668},"source":"# Start writing code here...\n\n# need to change the way the weights are reshaped\nclass LunaFeedforward:\n    def __init__(self, architecture, random=None, weights=None):\n        self.params = {'H': architecture['width'],\n                       'L': architecture['hidden_layers'],\n                       'D_in': architecture['input_dim'],\n                       'D_out': architecture['output_dim'],\n                       'D_final_out' : architecture['final_output_dim'],\n                       'activation_type': architecture['activation_fn_type'],\n                       'activation_params': architecture['activation_fn_params']}\n\n        \n        self.D = (  (architecture['input_dim'] * architecture['width'] + architecture['width']) #number of weights/biases between input and first hidden layer\n                  + (architecture['output_dim'] * architecture['width'] + architecture['output_dim']) #number of weights/biases between last hidden layer and output\n                  + (architecture['hidden_layers'] - 1) * (architecture['width']**2 + architecture['width']) #number of weights/biases between each pair of hidden layers\n                 )\n\n        if random is not None:\n            self.random = random\n        else:\n            self.random = np.random.RandomState(0)\n\n        self.h = architecture['activation_fn']\n\n        if weights is None:\n            self.weights = self.random.normal(0, 1, size=(1, self.D))\n        else:\n            self.weights = weights\n\n        self.objective_trace = np.empty((1, 1))\n        self.weight_trace = np.empty((1, self.D))\n\n    # for LUNA, need to discard aux functions\n    def forward(self, weights, x, final_layer_out=False):\n        ''' Forward pass given weights and input '''\n        H = self.params['H']\n        D_in = self.params['D_in']\n        D_out = self.params['D_out']\n\n        assert weights.shape[1] == self.D\n\n        #making sure the middle dimension is D_in\n        if len(x.shape) == 2:\n            assert x.shape[0] == D_in\n            x = x.reshape((1, D_in, -1))\n        else:\n            assert x.shape[1] == D_in\n\n        weights = weights.T\n\n        #input to first hidden layer\n        W = weights[:H * D_in].T.reshape((-1, H, D_in))\n        b = weights[H * D_in:H * D_in + H].T.reshape((-1, H, 1))\n        input = self.h(np.matmul(W, x) + b)\n        index = H * D_in + H\n\n        assert input.shape[1] == H\n\n        #additional hidden layers\n        for _ in range(self.params['L'] - 1):\n            before = index\n            W = weights[index:index + H * H].T.reshape((-1, H, H))\n            index += H * H\n            b = weights[index:index + H].T.reshape((-1, H, 1))\n            index += H\n            output = np.matmul(W, input) + b\n            input = self.h(output) #apply activation func\n\n            assert input.shape[1] == H\n\n        #last hidden layer\n        final_layer = np.array(input, copy = True)\n\n        #output layer\n        W = weights[index:index + H * D_out].T.reshape((-1, D_out, H))\n        b = weights[index + H * D_out:].T.reshape((-1, D_out, 1))\n        output = np.matmul(W, input) + b\n        assert output.shape[1] == self.params['D_out']\n\n        \n        # final_layer_out = True when we are using forward to predict, NOT training\n        if final_layer_out:\n            return final_layer\n        # final_layer_out\n        else:\n            return output\n\n    def get_aux_funcs(self, W):\n        res_w, res_b = [], []\n        D, D_out, H = self.D, self.params['D_out'], self.params['H']\n        index = D - (D_out*H + D_out) - 1\n        for m in range(D_out):\n            w_m = W[0][index + H*m:index + H*(m+1)]\n            b_m = W[0][index + H*(m+1)]\n            index += 1\n            res_w.append(w_m)      \n            res_b.append(b_m)\n            \n        #res_w is a list of lists\n        #res_b is a list of numbers\n        return res_w, res_b\n\n    def similarity_score(self, W):\n        # f is a vectorized function, x is a vector\n        # needs to return a vector\n        def grad_finite_diff(f, x):\n            dx = 0.001\n            return (f(x + dx) - f(x))/dx\n\n        def cos_sim_sq(fi, fj, x): \n            # returns 1 when fi parallel to fj\n            # returns 0 when fi perpendicular to fj\n            grad_i = grad_finite_diff(fi, x)\n            grad_j = grad_finite_diff(fj, x)\n            numerator = np.dot(grad_i, grad_j.T)**2\n            denominator = np.dot(grad_i, grad_i.T) * np.dot(grad_j, grad_j.T)\n            frac = numerator/denominator\n            return frac\n\n        # calculate square of cosine similarity for each pair of aux functions\n        score = 0\n        final_hidden_layer = self.forward(W, x_train, final_layer_out=True, aux = True)\n        aux_func_weights, aux_func_biases = self.get_aux_funcs(W)\n        for i in range(self.params['D_out']):\n            w_i = aux_func_weights[i]\n            b_i = aux_func_biases[i]\n            f_i = lambda x : np.matmul(w_i, x) + b_i#applying aux weights w_i to last hidden layer\n            for j in range(i + 1, self.params['D_out']):\n                w_j = aux_func_weights[j]\n                b_j = aux_func_biases[j]\n                f_j = lambda x : np.matmul(w_j, x) + b_j#applying aux weights w_j to last hidden layer\n                score += cos_sim_sq(f_i, f_j, final_hidden_layer)\n        \n        return score\n    \n    def mean_mean_sq_error(self, W):\n        aux_outputs = self.forward(W, x_train) #shape = (1,10,12)\n        Y = np.tile(y_train, D_out).reshape(1, D_out, y_train.shape[1])\n\n        # calculate squared error for each aux regressor, take mean\n        mse = np.mean(np.linalg.norm(Y - aux_outputs, axis=1)**2)\n\n        return mse\n\n    # for LUNA, this needs to use aux functions\n    def make_objective(self, x_train, y_train, reg_param):\n\n        def objective(W, t):\n            # L_luna(model) = L_fit(model) - lambda*L_diverse(model)\n            lambda_ = 0.1\n            gamma_ = reg_param\n            L_sim = lambda_*self.similarity_score(W)\n\n            regularization_penalty = gamma_*np.linalg.norm(W)**2\n            mean_mse = self.mean_mean_sq_error(W)\n            L_fit = mean_mse - regularization_penalty\n            return L_fit - L_sim\n\n        return objective, grad(objective)\n\n    # for LUNA, this needs to use aux functions\n    # by default, the fit function applies l2-regularization with a param of 0.1\n    def fit(self, x_train, y_train, params, reg_param = 0.1):\n\n        assert x_train.shape[0] == self.params['D_in']\n        #assert y_train.shape[0] == self.params['D_final_out']\n\n        ### make objective function for training\n        self.objective, self.gradient = self.make_objective(x_train, y_train, reg_param)\n\n        ### set up optimization\n        step_size = 0.01\n        max_iteration = 5000\n        check_point = 100\n        weights_init = self.weights.reshape((1, -1))\n        mass = None\n        optimizer = 'adam' # DEFAULT. CHANGE IN PARAMS\n        opt_gradient = self.gradient # DEFAULT. CHANGE IN PARAMS\n        random_restarts = 5\n\n        if 'step_size' in params.keys():\n            step_size = params['step_size']\n        if 'max_iteration' in params.keys():\n            max_iteration = params['max_iteration']\n        if 'check_point' in params.keys():\n            self.check_point = params['check_point']\n        if 'init' in params.keys():\n            weights_init = params['init']\n        if 'call_back' in params.keys():\n            call_back = params['call_back']\n        if 'mass' in params.keys():\n            mass = params['mass']\n        if 'optimizer' in params.keys():\n            optimizer = params['optimizer']\n        if  'opt_gradient' in params.keys():\n            gradient = params['opt_gradient']\n        if 'random_restarts' in params.keys():\n            random_restarts = params['random_restarts']\n\n        def call_back(weights, iteration, g):\n            ''' Actions per optimization step '''\n            objective = self.objective(weights, iteration)\n            self.objective_trace = np.vstack((self.objective_trace, objective))\n            self.weight_trace = np.vstack((self.weight_trace, weights))\n            if iteration % check_point == 0:\n                print(\"Iteration {} lower bound {}; gradient mag: {}\".format(iteration, objective, np.linalg.norm(self.gradient(weights, iteration))))\n\n        ### train with random restarts\n        optimal_obj = 1e16\n        optimal_weights = self.weights\n\n        # AM205 CREW: gradient is used here\n        for i in range(random_restarts):\n\n            # this is the default\n            if optimizer == 'adam':\n                adam(opt_gradient, weights_init, step_size=step_size, num_iters=max_iteration, callback=call_back)\n            \n            local_opt = np.min(self.objective_trace[-100:])\n            if local_opt < optimal_obj:\n                opt_index = np.argmin(self.objective_trace[-100:])\n                self.weights = self.weight_trace[-100:][opt_index].reshape((1, -1))\n            weights_init = self.random.normal(0, 1, size=(1, self.D))\n\n        self.objective_trace = self.objective_trace[1:]\n        self.weight_trace = self.weight_trace[1:]\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00001-239e7339-1b49-4fca-a770-834e1990a7da","output_cleared":false,"source_hash":"96ac48f1","execution_millis":2,"execution_start":1606150915552},"source":"###relu activation\nactivation_fn_type = 'relu'\nactivation_fn = lambda x: np.maximum(np.zeros(x.shape), x)\n\n\n###neural network model design choices\nwidth = 7\nhidden_layers = 2\ninput_dim = 1\noutput_dim = 10 #number of auxiliary functions\nfinal_output_dim = 1\n\n\narchitecture = {'width': width,\n               'hidden_layers': hidden_layers,\n               'input_dim': input_dim,\n               'output_dim': output_dim,\n               'final_output_dim' : final_output_dim,\n               'activation_fn_type': 'relu',\n               'activation_fn_params': 'rate=1',\n               'activation_fn': activation_fn}\n\n#set random state to make the experiments replicable\nrand_state = 0\nrandom = np.random.RandomState(rand_state)\n\n#instantiate a Feedforward neural network object\nnn = LunaFeedforward(architecture, random=random)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-537f0bc0-567a-4858-8656-64e5eba2b69c","output_cleared":false,"source_hash":"206ed2b4","execution_millis":1,"execution_start":1606150918765},"source":"df = pd.read_csv(\"HW8_data.csv\")\nx_train = np.array(df[\"x\"])\ny_train = np.array(df[\"y\"])\nx_test = np.linspace(x_train.min()-1,x_train.max()+1,200)\n\nx_train = x_train.reshape((1, -1))\ny_train = y_train.reshape((1, -1))\nx_test = x_test.reshape((1, -1))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-cf384fed-01a4-4a4b-a3b5-edd790a295ba","output_cleared":false,"source_hash":"60244edd","execution_millis":1,"execution_start":1606150931256},"source":"#set random state to make the experiments replicable\nrand_state = 0\nrandom = np.random.RandomState(rand_state)\n\n###define design choices in gradient descent\n\n#from having run this a few times, seems like 2500 iterations is a good amount. but this may depend on the other params :/\n\nparams = {'step_size':1e-3, \n          'max_iteration':15000, \n          'random_restarts':1,\n          'optimizer':'adam'}\n#nn.fit(x_train, y_train, params)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-7ef57b1d-8f3a-481d-a95d-1301a520ee9d","output_cleared":false,"source_hash":"95edb2f7","execution_millis":1613859,"execution_start":1606165720511},"source":"from utils import run_toy_nn\n\nrun_toy_nn(LunaFeedforward,architecture,params,random,x_train,y_train,x_test)","execution_count":null,"outputs":[{"name":"stdout","text":"Iteration 0 lower bound [[888.68161547]]; gradient mag: 2081.130319109057\nIteration 100 lower bound [[469.74761216]]; gradient mag: 1334.2646008789827\nIteration 200 lower bound [[253.77959224]]; gradient mag: 895.8900655879667\nIteration 300 lower bound [[135.40309107]]; gradient mag: 613.9669517530178\nIteration 400 lower bound [[68.86918655]]; gradient mag: 417.13410813304665\nIteration 500 lower bound [[32.08817985]]; gradient mag: 273.39177361827285\nIteration 600 lower bound [[12.1112154]]; gradient mag: 169.15606717844253\nIteration 700 lower bound [[0.76499986]]; gradient mag: 97.5614073210017\nIteration 800 lower bound [[-6.68961698]]; gradient mag: 52.40698083338497\nIteration 900 lower bound [[-12.08150268]]; gradient mag: 26.403174251733983\nIteration 1000 lower bound [[-15.90354843]]; gradient mag: 12.797853524547804\nIteration 1100 lower bound [[-19.00191413]]; gradient mag: 7.730563331027035\nIteration 1200 lower bound [[-22.03030252]]; gradient mag: 6.08328051605766\nIteration 1300 lower bound [[-25.14451385]]; gradient mag: 5.416946634506356\nIteration 1400 lower bound [[-28.35706534]]; gradient mag: 5.206218818087173\nIteration 1500 lower bound [[-31.6681508]]; gradient mag: 5.114606673424366\nIteration 1600 lower bound [[-35.07297677]]; gradient mag: 5.241008598167944\nIteration 1700 lower bound [[-38.59809197]]; gradient mag: 5.201823639451657\nIteration 1800 lower bound [[-42.25404039]]; gradient mag: 5.269492426085581\nIteration 1900 lower bound [[-46.0381375]]; gradient mag: 5.247123398369454\nIteration 2000 lower bound [[-49.95553399]]; gradient mag: 5.321170670224198\nIteration 2100 lower bound [[-54.00793297]]; gradient mag: 5.403826065346639\nIteration 2200 lower bound [[-58.19751248]]; gradient mag: 5.613532851025825\nIteration 2300 lower bound [[-62.52740018]]; gradient mag: 5.611246237898234\nIteration 2400 lower bound [[-67.00128611]]; gradient mag: 5.728742649375977\nIteration 2500 lower bound [[-71.62308541]]; gradient mag: 5.867315180880691\nIteration 2600 lower bound [[-76.39677156]]; gradient mag: 5.987268380738479\nIteration 2700 lower bound [[-81.32629799]]; gradient mag: 6.07376898696132\nIteration 2800 lower bound [[-86.41570171]]; gradient mag: 6.187882230529887\nIteration 2900 lower bound [[-91.66903635]]; gradient mag: 6.309562059511264\nIteration 3000 lower bound [[-97.09747723]]; gradient mag: 6.473528792276171\nIteration 3100 lower bound [[-102.75761132]]; gradient mag: 6.611069001989997\nIteration 3200 lower bound [[-108.58789352]]; gradient mag: 6.7598176005770725\nIteration 3300 lower bound [[-114.60286104]]; gradient mag: 6.92051800953713\nIteration 3400 lower bound [[-120.81172849]]; gradient mag: 7.090282490893719\nIteration 3500 lower bound [[-127.22322231]]; gradient mag: 7.267315578324809\nIteration 3600 lower bound [[-133.84596725]]; gradient mag: 7.44880193908748\nIteration 3700 lower bound [[-140.64184222]]; gradient mag: 7.621437870480865\nIteration 3800 lower bound [[-147.64113149]]; gradient mag: 7.796167223259734\nIteration 3900 lower bound [[-154.85907091]]; gradient mag: 7.969393067922424\nIteration 4000 lower bound [[-162.25023899]]; gradient mag: 8.141978353740999\nIteration 4100 lower bound [[-169.80422887]]; gradient mag: 8.303069668331709\nIteration 4200 lower bound [[-177.54043388]]; gradient mag: 14.776459358692222\nIteration 4300 lower bound [[-185.38687082]]; gradient mag: 8.65519014431886\nIteration 4400 lower bound [[-193.39773922]]; gradient mag: 8.833557980533845\nIteration 4500 lower bound [[-201.54118789]]; gradient mag: 9.064107695206408\nIteration 4600 lower bound [[-209.81716134]]; gradient mag: 9.205263463710898\nIteration 4700 lower bound [[-218.25346015]]; gradient mag: 12.53193968461915\nIteration 4800 lower bound [[-226.80404435]]; gradient mag: 9.577535011199155\nIteration 4900 lower bound [[-235.52404579]]; gradient mag: 33.69203950780431\nIteration 5000 lower bound [[-244.36959302]]; gradient mag: 9.944588943924145\nIteration 5100 lower bound [[-253.38449067]]; gradient mag: 10.39542056579481\nIteration 5200 lower bound [[-262.51950161]]; gradient mag: 10.310630790218394\nIteration 5300 lower bound [[-271.82524365]]; gradient mag: 11.318463223342121\nIteration 5400 lower bound [[-281.24658805]]; gradient mag: 10.657502495269465\nIteration 5500 lower bound [[-290.83729715]]; gradient mag: 12.515242036156764\nIteration 5600 lower bound [[-300.54336537]]; gradient mag: 11.009241826458044\nIteration 5700 lower bound [[-310.4144141]]; gradient mag: 33.674557230311386\nIteration 5800 lower bound [[-320.39026574]]; gradient mag: 11.37031429295597\nIteration 5900 lower bound [[-330.5323673]]; gradient mag: 11.53650530817424\nIteration 6000 lower bound [[-340.7842088]]; gradient mag: 11.799286070036255\nIteration 6100 lower bound [[-351.18678807]]; gradient mag: 11.896373489923086\nIteration 6200 lower bound [[-361.72189969]]; gradient mag: 12.3841429955699\nIteration 6300 lower bound [[-372.3880628]]; gradient mag: 12.249498202159684\nIteration 6400 lower bound [[-383.19778996]]; gradient mag: 16.111168201386075\nIteration 6500 lower bound [[-394.12128071]]; gradient mag: 12.60235316174807\nIteration 6600 lower bound [[-405.20711304]]; gradient mag: 59.86810517878701\nIteration 6700 lower bound [[-416.39903593]]; gradient mag: 12.948303600652565\nIteration 6800 lower bound [[-427.7626049]]; gradient mag: 34.79430229748629\nIteration 6900 lower bound [[-439.22469159]]; gradient mag: 13.299779496506456\nIteration 7000 lower bound [[-450.86602737]]; gradient mag: 13.479691158037241\nIteration 7100 lower bound [[-462.6129648]]; gradient mag: 13.644163479069158\nIteration 7200 lower bound [[-474.50235643]]; gradient mag: 13.821058731906891\nIteration 7300 lower bound [[-486.57024094]]; gradient mag: 25.09286373942238\nIteration 7400 lower bound [[-498.71945214]]; gradient mag: 14.1632102616717\nIteration 7500 lower bound [[-511.05309644]]; gradient mag: 14.339599022156278\nIteration 7600 lower bound [[-523.51669019]]; gradient mag: 16.423130964176583\nIteration 7700 lower bound [[-536.11628154]]; gradient mag: 14.684302416353006\nIteration 7800 lower bound [[-548.89657213]]; gradient mag: 100.99604624019919\nIteration 7900 lower bound [[-561.782581]]; gradient mag: 15.030756738407954\nIteration 8000 lower bound [[-574.85353195]]; gradient mag: 15.207286864449166\nIteration 8100 lower bound [[-588.07046639]]; gradient mag: 34.43994947410745\nIteration 8200 lower bound [[-601.40927904]]; gradient mag: 15.562350768321005\nIteration 8300 lower bound [[-614.95121491]]; gradient mag: 15.730797804576556\nIteration 8400 lower bound [[-628.64598961]]; gradient mag: 36.566495705379275\nIteration 8500 lower bound [[-642.47419796]]; gradient mag: 16.08729098358694\nIteration 8600 lower bound [[-656.51187926]]; gradient mag: 16.25968808557535\nIteration 8700 lower bound [[-670.69370062]]; gradient mag: 24.125499659523065\nIteration 8800 lower bound [[-685.02892506]]; gradient mag: 16.620299366126492\nIteration 8900 lower bound [[-699.58268006]]; gradient mag: 16.79753952510044\nIteration 9000 lower bound [[-714.2764772]]; gradient mag: 20.57674928846581\nIteration 9100 lower bound [[-729.15005602]]; gradient mag: 17.17342081591156\nIteration 9200 lower bound [[-744.25680883]]; gradient mag: 18.35522953791274\nIteration 9300 lower bound [[-759.49537638]]; gradient mag: 17.54310705517976\nIteration 9400 lower bound [[-774.95261774]]; gradient mag: 17.744194313221627\nIteration 9500 lower bound [[-790.63765599]]; gradient mag: 212.7298450644752\nIteration 9600 lower bound [[-806.48456148]]; gradient mag: 18.114316326855356\nIteration 9700 lower bound [[-822.5668064]]; gradient mag: 18.331303233683744\nIteration 9800 lower bound [[-838.87511091]]; gradient mag: 221.1081238741897\nIteration 9900 lower bound [[-855.34331552]]; gradient mag: 18.717254582837246\nIteration 10000 lower bound [[-872.05200373]]; gradient mag: 18.922679072136745\nIteration 10100 lower bound [[-889.01658901]]; gradient mag: 88.40705636181006\nIteration 10200 lower bound [[-906.05610025]]; gradient mag: 20.260708019323694\nIteration 10300 lower bound [[-923.31715753]]; gradient mag: 19.486172705609853\nIteration 10400 lower bound [[-940.82209088]]; gradient mag: 19.667080675648016\nIteration 10500 lower bound [[-958.50923298]]; gradient mag: 50.169816128838306\nIteration 10600 lower bound [[-976.30431529]]; gradient mag: 20.03963542024436\nIteration 10700 lower bound [[-994.32667842]]; gradient mag: 20.217646518926255\nIteration 10800 lower bound [[-1012.57194928]]; gradient mag: 60.02779553831341\nIteration 10900 lower bound [[-1030.88452356]]; gradient mag: 20.99764410413481\nIteration 11000 lower bound [[-1049.38946457]]; gradient mag: 20.754088902303373\nIteration 11100 lower bound [[-1068.107405]]; gradient mag: 24.288756814963328\nIteration 11200 lower bound [[-1086.96616601]]; gradient mag: 21.133010076443938\nIteration 11300 lower bound [[-1106.00461661]]; gradient mag: 59.7829752749496\nIteration 11400 lower bound [[-1125.19259949]]; gradient mag: 82.22654888825355\nIteration 11500 lower bound [[-1144.53355721]]; gradient mag: 243.1295142467705\nIteration 11600 lower bound [[-1164.04523026]]; gradient mag: 24.593999337666517\nIteration 11700 lower bound [[-1183.69222116]]; gradient mag: 23.671687697670297\nIteration 11800 lower bound [[-1203.50035861]]; gradient mag: 30.96244909335883\nIteration 11900 lower bound [[-1223.45880328]]; gradient mag: 23.462328059979665\nIteration 12000 lower bound [[-1243.56436075]]; gradient mag: 52.30512933224256\nIteration 12100 lower bound [[-1263.82202731]]; gradient mag: 38.059432596215466\nIteration 12200 lower bound [[-1284.2200095]]; gradient mag: 22.917981224630836\nIteration 12300 lower bound [[-1304.77344728]]; gradient mag: 25.115711751041115\nIteration 12400 lower bound [[-1325.48789184]]; gradient mag: 79.47269301224964\nIteration 12500 lower bound [[-1346.35904663]]; gradient mag: 85.03515532994317\nIteration 12600 lower bound [[-1367.37833542]]; gradient mag: 39.483397735452314\nIteration 12700 lower bound [[-1388.55299687]]; gradient mag: 27.840312696077838\nIteration 12800 lower bound [[-1409.88443709]]; gradient mag: 33.85772062397981\nIteration 12900 lower bound [[-1431.34704315]]; gradient mag: 25.63455080853138\nIteration 13000 lower bound [[-1452.97286279]]; gradient mag: 73.66648447259654\nIteration 13100 lower bound [[-1474.74048681]]; gradient mag: 44.21362289764025\nIteration 13200 lower bound [[-1496.65742134]]; gradient mag: 149.3187999429768\nIteration 13300 lower bound [[-1518.7271635]]; gradient mag: 86.26215485368238\nIteration 13400 lower bound [[-1540.95017106]]; gradient mag: 167.63473477924012\nIteration 13500 lower bound [[-1563.32724572]]; gradient mag: 47.64131289994394\nIteration 13600 lower bound [[-1585.84656566]]; gradient mag: 86.92833338629211\nIteration 13700 lower bound [[-1608.49312647]]; gradient mag: 534.6576991537598\nIteration 13800 lower bound [[-1631.34302702]]; gradient mag: 394.1304084911842\nIteration 13900 lower bound [[-1654.31451133]]; gradient mag: 447.2229669044908\nIteration 14000 lower bound [[-1677.43819004]]; gradient mag: 26.210312015199815\nIteration 14100 lower bound [[-1700.71214237]]; gradient mag: 54.764385263226735\nIteration 14200 lower bound [[-1724.13105046]]; gradient mag: 86.83410079646498\nIteration 14300 lower bound [[-1747.70204047]]; gradient mag: 110.37504150122201\nIteration 14400 lower bound [[-1771.4126093]]; gradient mag: 92.30168314959651\nIteration 14500 lower bound [[-1795.23037815]]; gradient mag: 650.26408092429\nIteration 14600 lower bound [[-1819.28507192]]; gradient mag: 457.4666668605783\nIteration 14700 lower bound [[-1843.47338773]]; gradient mag: 29.517268749363172\nIteration 14800 lower bound [[-1867.78443746]]; gradient mag: 59.039187353645936\nIteration 14900 lower bound [[-1892.26315708]]; gradient mag: 53.48021864550085\n(200, 1)\n","output_type":"stream"},{"output_type":"error","ename":"NameError","evalue":"name 'plt' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-209-727e8cd71ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_toy_nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_toy_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLunaFeedforward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/work/utils.py\u001b[0m in \u001b[0;36mrun_toy_nn\u001b[0;34m(nn_model, architecture, params, random, x_train, y_train, x_test)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#visualize the function learned by the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'learned neural network function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"]}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00006-bb6d79ba-bd0d-40ef-978e-1f1b552c9c42","output_cleared":false,"source_hash":"af409688","execution_start":1606150125178,"execution_millis":0},"source":"nn = LunaFeedforward(architecture, random=random)\n\n#fit my neural network to minimize MSE on the given data\nnn.fit(x_train, y_train, params)\n\n#predict on the test x-values\ny_test_pred = nn.forward(nn.weights, x_test)\nprint(x_test.flatten().reshape(-1,1).shape)\n#visualize the function learned by the neural network\nplt.scatter(x_train.flatten(), y_train.flatten(), color='black', label='data')\n\n\nplt.plot(x_test.flatten(), y_test_pred.flatten(), color='red', label='learned neural network function')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'y_test_pred' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-190-546f94c60369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'y_test_pred' is not defined"]}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-e41a4740-1625-434f-9de5-9da657ead16b","output_cleared":false,"source_hash":"14d696b5","execution_millis":1,"execution_start":1606149602913},"source":"# Recommendation: make X and Y inputs to the train function instead of the NLM constructor \n# so that the same model could be re-trained on new datasets \n\nfrom scipy.stats import multivariate_normal\nfrom feed_forward import Feedforward\n\nclass LUNA():\n    \"\"\"\n    Fits LUNA Model\n    \n    Model Assumptions\n     - Weights distributed normally\n     - Ys distributed normally\n\n\n     How to use:\n      - run train() to create: \n            a) the NN MLE weights, found in self.ff.weights \n            b) self.posterior samples, the distribution for the weights in the last layer of NLM\n     \n      - run predict() to get distribution of ys, given x test\n    \"\"\"\n    def __init__(self,X,Y,prior_var,y_noise_var,architecture, random_state):\n\n        self.lff = Feedforward(architecture, random = random_state) #CHANGE\n        \n        self.X = X # X training\n        self.Y = Y # Y training data\n        \n        self.prior_var = prior_var # prior variance final layer weights\n        self.y_noise_var = y_noise_var # variance of noise, distributed normally\n\n    def train(self):\n        \n        # Fit Weights\n        self.lff.fit(self.X, self.Y, params)\n\n        # Transform X with Feature Map for Bayes Reg\n            #evaluated NN up to lst layer by setting final_layer_out to True\n        fm_x_matrix = self.lff.forward(self.lff.weights, self.X, final_layer_out=True)\n        \n        # Conduct Bayes Reg on Final Layer \n        self.posterior_samples = bh.get_bayes_lr_posterior(self.prior_var,\n                                                        self.y_noise_var,\n                                                        fm_x_matrix.T[:,:,0], #fm_x_matrix, \n                                                        self.Y.T,\n                                                        samples=100)\n\n        print(\"Done Training\")\n    \n    def predict(self,X_test):\n\n        # forward pass up to last layer\n        fm_x_matrix = self.lff.forward(self.lff.weights, X_test, final_layer_out=True)\n        print(fm_x_matrix.T[:,:,0].shape)\n        # get posterior predictives, posterior of final layer weights\n        return bh.get_bayes_lr_posterior_predictives(self.y_noise_var,self.posterior_samples,fm_x_matrix.T[:,:,0])\n\n    def get_aux_funcs(self, W):\n        res_w, res_b = [], []\n        D, D_out, H = self.D, self.params['D_out'], self.params['H']\n        index = D - (D_out*H + D_out) - 1\n        for m in range(D_out):\n            w_m = W[0][index + H*m:index + H*(m+1)]\n            b_m = W[0][index + H*(m+1)]\n            index += 1\n            res_w.append(w_m)      \n            res_b.append(b_m)\n            \n        #res_w is a list of lists\n        #res_b is a list of numbers\n        return res_w, res_b\n\n\n    def similarity_score(self, W):\n        # f is a vectorized function, x is a vector\n        # needs to return a vector\n        def grad_finite_diff(f, x):\n            dx = 0.001\n            return (f(x + dx) - f(x))/dx\n\n        def cos_sim_sq(fi, fj, x): \n            # returns 1 when fi parallel to fj\n            # returns 0 when fi perpendicular to fj\n            grad_i = grad_finite_diff(fi, x)\n            grad_j = grad_finite_diff(fj, x)\n            numerator = np.dot(grad_i, grad_j.T)**2\n            denominator = np.dot(grad_i, grad_i.T) * np.dot(grad_j, grad_j.T)\n            frac = numerator/denominator\n            return frac\n\n        # calculate square of cosine similarity for each pair of aux functions\n        score = 0\n        final_hidden_layer = self.forward(W, x_train, final_layer_out=True, aux = True)\n        aux_func_weights, aux_func_biases = self.get_aux_funcs(W)\n        for i in range(self.params['D_out']):\n            w_i = aux_func_weights[i]\n            b_i = aux_func_biases[i]\n            f_i = lambda x : np.matmul(w_i, x) + b_i#applying aux weights w_i to last hidden layer\n            for j in range(i + 1, self.params['D_out']):\n                w_j = aux_func_weights[j]\n                b_j = aux_func_biases[j]\n                f_j = lambda x : np.matmul(w_j, x) + b_j#applying aux weights w_j to last hidden layer\n                score += cos_sim_sq(f_i, f_j, final_hidden_layer)\n        \n        return score\n    \n    def mean_mean_sq_error(self, W):\n        aux_outputs = self.forward(W, x_train) #shape = (1,10,12)\n        Y = np.tile(y_train, D_out).reshape(1, D_out, y_train.shape[1])\n\n        # calculate squared error for each aux regressor, take mean\n        mse = np.mean(np.linalg.norm(Y - aux_outputs, axis=1)**2)\n\n        return mse\n\n    # for LUNA, this needs to use aux functions\n    def make_objective(self, x_train, y_train, reg_param):\n\n        def objective(W, t):\n            # L_luna(model) = L_fit(model) - lambda*L_diverse(model)\n            lambda_ = 0.1\n            gamma_ = reg_param\n            L_sim = lambda_*self.similarity_score(W)\n\n            regularization_penalty = gamma_*np.linalg.norm(W)**2\n            mean_mse = self.mean_mean_sq_error(W)\n            L_fit = mean_mse - regularization_penalty\n            return L_fit - L_sim\n\n        return objective, grad(objective)\n\n        \n\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00007-1f8546e0-6a3d-48b5-be16-be1b6be847fa","output_cleared":false,"source_hash":"e29ec874","execution_millis":1,"execution_start":1606149606331},"source":"# test\nprior_var = 1.0\ny_var = 2.0\ntest_luna = LUNA(x_train, y_train, prior_var,y_var, architecture, random_state = np.random.RandomState(0))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00008-baca12e5-eb43-491e-b5c2-691bcf827570","output_cleared":false,"source_hash":"e865b039","execution_millis":105801,"execution_start":1606149607552},"source":"test_luna.train()\nposterior_predictives, posterior_predictive_samples = test_luna.predict(x_test)","execution_count":null,"outputs":[{"name":"stdout","text":"Iteration 0 lower bound [[4317.13123622]]; gradient mag: 8006.452126483383\nIteration 100 lower bound [[1702.14720813]]; gradient mag: 4275.369428724128\nIteration 200 lower bound [[721.83507175]]; gradient mag: 2409.812419489686\nIteration 300 lower bound [[324.56556835]]; gradient mag: 1415.3510749089103\nIteration 400 lower bound [[156.7219972]]; gradient mag: 857.252774400587\nIteration 500 lower bound [[83.17189534]]; gradient mag: 528.4369113751939\nIteration 600 lower bound [[49.67515363]]; gradient mag: 326.46962757618365\nIteration 700 lower bound [[33.24086795]]; gradient mag: 203.06173792907356\nIteration 800 lower bound [[24.54164613]]; gradient mag: 125.91667112071974\nIteration 900 lower bound [[20.3081267]]; gradient mag: 29.796996710693588\nDone Training\n(200, 5)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00009-8aa608b6-a09d-4bbd-9462-4134be2b8565","output_cleared":false,"source_hash":"5142944","execution_millis":222,"execution_start":1606149716083},"source":"viz_pp_samples(x_train, y_train,x_test.flatten(),posterior_predictive_samples,\"LUNA test\")","execution_count":null,"outputs":[{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00010-000b4fe2-ea12-4ab7-95c9-9ca2e3ad80db","output_cleared":false,"source_hash":"b623e53d"},"source":"# LUNA v1.5\nfrom feed_forward import Feedforward\nfrom nlm import NLM\n\nclass LUNA():\n    \"\"\"\n    Fits LUNA Model\n    \n    Model Assumptions\n     - Weights distributed normally\n     - Ys distributed normally\n\n     How to use:\n      - run train() to create: \n            a) the NN MLE weights, found in self.ff.weights \n            b) self.posterior samples, the distribution for the weights in the last layer of NLM\n     \n      - run predict() to get distribution of ys, given x test\n    \"\"\"\n    def __init__(self,prior_var,y_noise_var,architecture, random_state):\n\n        self.ff = Feedforward(architecture, random = random_state)\n        \n        self.prior_var = prior_var # prior variance final layer weights\n        self.y_noise_var = y_noise_var # variance of noise, distributed normally\n\n    def train(self, X, Y, params):\n        \n        # Fit Weights\n        self.ff.fit(X,Y, params)\n\n        # Transform X with Feature Map for Bayes Reg\n            #evaluated NN up to lst layer by setting final_layer_out to True\n        final_layer = self.ff.forward(self.ff.weights, X, final_layer_out=True)\n        \n        # Conduct Bayes Reg on Final Layer \n        self.posterior_samples = bh.get_bayes_lr_posterior(self.prior_var,\n                                                        self.y_noise_var,\n                                                        final_layer.T[:,:,0], \n                                                        Y.T,\n                                                        samples=100)\n\n        print(\"Done Training\")\n    \n    def predict(self, X_test):\n\n        # forward pass up to last layer\n        fm_x_matrix = self.ff.forward(self.ff.weights, X_test, final_layer_out=True)\n        print(final_layer.T[:,:,0].shape)\n        # get posterior predictives, posterior of final layer weights\n        return bh.get_bayes_lr_posterior_predictives(self.y_noise_var,self.posterior_samples,final_layer.T[:,:,0])\n\n    def get_aux_funcs(self, W):\n        res_w, res_b = [], []\n        D, D_out, H = self.D, self.params['D_out'], self.params['H']\n        index = D - (D_out*H + D_out) - 1\n        for m in range(D_out):\n            w_m = W[0][index + H*m:index + H*(m+1)]\n            b_m = W[0][index + H*(m+1)]\n            index += 1\n            res_w.append(w_m)      \n            res_b.append(b_m)\n            \n        #res_w is a list of lists\n        #res_b is a list of numbers\n        return res_w, res_b\n\n\n    def similarity_score(self, W):\n        # f is a vectorized function, x is a vector\n        # needs to return a vector\n        def grad_finite_diff(f, x):\n            dx = 0.001\n            return (f(x + dx) - f(x))/dx\n\n        def cos_sim_sq(fi, fj, x): \n            # returns 1 when fi parallel to fj\n            # returns 0 when fi perpendicular to fj\n            grad_i = grad_finite_diff(fi, x)\n            grad_j = grad_finite_diff(fj, x)\n            numerator = np.dot(grad_i, grad_j.T)**2\n            denominator = np.dot(grad_i, grad_i.T) * np.dot(grad_j, grad_j.T)\n            frac = numerator/denominator\n            return frac\n\n        # calculate square of cosine similarity for each pair of aux functions\n        score = 0\n        final_hidden_layer = self.forward(W, x_train, final_layer_out=True, aux = True)\n        aux_func_weights, aux_func_biases = self.get_aux_funcs(W)\n        for i in range(self.params['D_out']):\n            w_i = aux_func_weights[i]\n            b_i = aux_func_biases[i]\n            f_i = lambda x : np.matmul(w_i, x) + b_i#applying aux weights w_i to last hidden layer\n            for j in range(i + 1, self.params['D_out']):\n                w_j = aux_func_weights[j]\n                b_j = aux_func_biases[j]\n                f_j = lambda x : np.matmul(w_j, x) + b_j#applying aux weights w_j to last hidden layer\n                score += cos_sim_sq(f_i, f_j, final_hidden_layer)\n        \n        return score\n    \n    def mean_mean_sq_error(self, W):\n        aux_outputs = self.forward(W, x_train) #shape = (1,10,12)\n        Y = np.tile(y_train, D_out).reshape(1, D_out, y_train.shape[1])\n\n        # calculate squared error for each aux regressor, take mean\n        mse = np.mean(np.linalg.norm(Y - aux_outputs, axis=1)**2)\n\n        return mse\n\n    # for LUNA, this needs to use aux functions\n    def make_objective(self, x_train, y_train, reg_param):\n\n        def objective(W, t):\n            # L_luna(model) = L_fit(model) - lambda*L_diverse(model)\n            lambda_ = 0.1\n            gamma_ = reg_param\n            L_sim = lambda_*self.similarity_score(W)\n\n            regularization_penalty = gamma_*np.linalg.norm(W)**2\n            mean_mse = self.mean_mean_sq_error(W)\n            L_fit = mean_mse - regularization_penalty\n            return L_fit - L_sim\n\n        return objective, grad(objective)","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"4e8d6942-e2c1-4a3f-b39c-e67d6ea196d8","deepnote_execution_queue":[]}}